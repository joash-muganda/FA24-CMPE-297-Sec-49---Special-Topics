{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsuOGJs4T-KX"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install trl peft bitsandbytes\n",
        "\n",
        "# Import necessary libraries\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Set parameters\n",
        "model_name = \"unsloth/Phi-3-medium-4k-instruct\"\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto-detection of data type (use Float16 for Tesla T4)\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n",
        "\n",
        "# Step 1: Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit\n",
        ")\n",
        "\n",
        "# Step 2: Add LoRA adapters using PEFT and prepare for quantized training\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Prepare the model for k-bit quantization training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Step 3: Prepare the dataset (ShareGPT style)\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"phi-3\",  # Phi-3 format for conversational style finetuning\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"}\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Load and format the dataset\n",
        "dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Step 4: Define training arguments and initialize the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # Packing can make training faster\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,  # Adjust the number of steps for training\n",
        "        learning_rate=2e-4,\n",
        "        bf16=True,  # Use BFloat16 for stability\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",  # Efficient optimizer for quantized training\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Step 5: Start training\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "\n",
        "# Step 7: Inference\n",
        "FastLanguageModel.for_inference(model)  # Enable faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Generate a story about a brave knight and a dragon.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add this for generation\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
        "decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated story:\", decoded_output[0])\n",
        "\n",
        "# Optionally: Stream inference for live token-by-token generation\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Inference\n",
        "FastLanguageModel.for_inference(model)  # Enable faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Generate a story about a brave knight and a dragon.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add this for generation\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate the story with the model\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
        "\n",
        "# Decode the generated tokens into text\n",
        "decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Print the generated story\n",
        "print(\"Generated story:\", decoded_output[0])\n",
        "\n",
        "# Optionally: Stream inference for live token-by-token generation\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXsSHrBMlhIl",
        "outputId": "8c250f5e-57b8-4f5e-f11e-5e2654001c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated story: Generate a story about a brave knight and a dragon. Once upon a time, in a kingdom far, far away, there lived a brave knight named Sir Cedric. Sir Cedric was known throughout the land for his courage and his unwaardable loyalty to the king. He had fought in many battles and had always emerged victorious.\n",
            "\n",
            "One day, the king summoned Sir Cedric to the castle. \"Sir Cedric,\" the king said, \"I have a quest for you. A terrible dragon has been terrorizing the nearby villages. It has been burning down farms and stealing livestock. I\n",
            "<|user|> Generate a story about a brave knight and a dragon.<|end|><|assistant|> Once upon a time, in a kingdom far, far away, there lived a brave knight named Sir Cedric. Sir Cedric was known throughout the land for his courage and his unwaardable loyalty to the king. He had fought in many battles and had always emerged victorious.\n",
            "\n",
            "One day, the king summoned Sir Cedric to the castle. \"Sir Cedric,\" the king said, \"I have a quest for you. A terrible dragon has been terrorizing the nearby villages. It has been burning down farms and stealing livestock. I\n"
          ]
        }
      ]
    }
  ]
}